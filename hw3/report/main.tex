\documentclass[9pt]{IEEEtran}

\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{url}
\usepackage{array}
\usepackage{textcomp}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{float}
\usepackage{gensymb}
\usepackage{longtable}
\usepackage{supertabular}
\usepackage{multicol}
\usepackage[justification=centering]{caption}
\usepackage{amsmath}
\usepackage{subcaption}

\usepackage[utf8x]{inputenc}

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\input{glyphtounicode}
\pdfgentounicode=1

\graphicspath{{./figures/}}
\DeclareGraphicsExtensions{.pdf,.png,.jpg,.eps}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor trig-gs}

% ============================================================================================

\title{\vspace{0ex}Logistic Regression}

\author{Alja≈æ Konec, 63190019\vspace{-4.0ex}}
\date{}
% ============================================================================================

\begin{document}

\maketitle

\section{Introduction}

Logistic regression is one of the most widely used machine learning algorithms for binary classification.
Its use can also be expanded to work on multi-class and ordinal classification problems.
Multi-class regression, also known as known as multinomial logistic regression, extends binary classification to alow for $K$ classes to be predicted.
Ordinal regression, on the other hand, is used when the classes have a natural, clear order to them.
Some such examples may include grades in school, customer satisfaction and modeling of human levels of preference (a scale from, say, 1-5 for "very poor" through "excellent").
In this report we first showcase the implementation details of multinomial logistic regression as well as ordinal logistic regression.
The next section describes how the coeficients of multinomial regression are related to the target classes.
In the final section we showcase an example of a data generating process that is better modeled for ordinal regression than for multinomial regression.


\section{Model implementation}

\subsection*{Multinomial logistic regression}
The multinomial logistic regression model differs from the binary logistic regression model in that it has $K$ sets of coefficients, one for each class.
To calculate the probability of a sample belonging to a certain class, we use the softmax function:
\begin{equation}
    P(y = k | x) = \frac{e^{\beta_k^T x}}{\sum_{j=1}^{K} e^{\beta_j^T x}}
\end{equation}
where $\beta_k$ is the set of coefficients for class $k$.
The loss functions is then defined as the negative log-likelihood over all samples:
\begin{equation}
    -logL(\beta) = -\sum_{i=1}^{N} \sum_{k=1}^{K} \delta_{ik} \log P(y = k | x_i)
\end{equation}
where $\delta_{ik}$ is the indicator function that is 1 if $y_i = k$ and 0 otherwise.
To account for the intercept term, we add a column of ones to the input matrix $X$.
We also add L2 regularization to the loss function.
The resulting $\beta$ is then a matrix of shape $(K, M)$ where $M$ is the number of features in the input matrix $X$.
For predicting the class of a sample, we simply take the class with the highest probability for that sample.

\subsection*{Ordinal logistic regression}

Ordinal logistic regression is a generalization of binary logistic regression to multiple classes that have a natural order to them.
We train $K-2$ thresholds $\tau$ that separate the classes.
These thresholds have to be ordered, i.e. $\tau_1 < \tau_2 < \ldots < \tau_{K-2}$.
We achieve this by appending $\Delta_{k-1}$ to the vector of coefficients $\beta$ for each threshold $\tau_k$.
And defining $\tau_0 = -\infty \leq \tau_1 = 0 \leq \tau_2 = \tau_1 + \Delta_1 \leq \ldots \leq \tau_{K-1} = \tau_{K-2} + \Delta_{K-2} \leq \tau_K = \infty$.
The probability vector of size $K$ is then defined (compontent-wise) as:
\begin{equation*}
    \begin{aligned}
        P(y = k | x) &= P(y <= k-1 | x) - P(y <= k | x)\\
         &= \frac{1}{1 + e^{\tau_k - \beta_k^T x}} - \frac{1}{1 + e^{\tau_{k-1} - \beta_k^T x}}
    \end{aligned}
\end{equation*}
Calculating the loss function is done in a similar way as for multinomial regression, but with the probability vectors defined above.
The resulting $\beta$ is then a matrix of shape $(K-1, M)$ where $M$ is the number of features in the input matrix $X$.

\section{Interpretation of coefficients}
To better understand the coefficients of the multinomial logistic regression model we train a classification model on A dataset of Baskeball Shot Types \cite{basket}.
The dataset contains 8 features: Shot Type, Competition, Player Type, Transition, TwoLegged, Movement, Angle and Distance.
The target variable is the Shot Type, which has 6 classes: Above Head, Dunk, Hook Shot, Layup, Tip-in and Other.
Competition, Player Type and Movement are categorical features that were one-hot encoded.
All features were then centered and scaled appropriately.
This resulted in 12 features and one target variable.

Since coeficients are an estimate they contain some uncertainty.
To model this uncertainty we use 100 repetitions of bootstrapping.
The results are shown in Figure \ref{fig:coef}.
\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{coefficients.pdf}
    \caption{Coefficients of the multinomial logistic regression model.}
    \label{fig:coef}
\end{figure}
A positive coefficient means that the feature is positively correlated with the target class.
It can be seen that Transition, Angle and Player Type have coefficients that are close to zero.
This makes intuitive sense as scoring a basket is not dependent on wether the shot was taken in transition.
The angle at wich the shot was taken is also not important as only very rare shots are taken at extreme angles.
All players practice shooting and therefoe the player type does not have a significant impact on the shot type.

With increasing distance from the basket, Tip-ins and Dunks become less likely or even impossible.
This is reflected in the very low negative coefficients for Distance in those classes.
The classic Above Head shot is more likely to be taken from a distance and as such has a high positive coefficient for Distance.
A similiarly high Distance coefficient can be seen for Other shots.

Another very important feature is movement and no movement.
An Above Head shot is very difficult to make while moving and as such has a very high negative coefficient for Movement.
The opposite is true for Layups, which are often taken while moving.
This is reflected in a positive coefficient for Drive Movement and a negative coefficient for No Movement.
The Hook Shot is also more likely to hit while stationary, which is reflected in a high coefficient for No Movement.

Dunks and Tip-ins are easier to make if the player is taller and can jump higher.
Therefore, the these classes are negativley correlated with the Under 14 Competitions as they are played by young players who are not fully grown yet.
Looking further into the Dunk class we can see that only the NBA competition has a positive coefficient asociated with it while the other competitions have negative coefficients.
From this we can gather that Dunking is more common in the NBA, which could be due to the fact that the NBA boasts the best players in the world and they are more likely to be able to succesfully perform a dunk.

The TwoLegged feature is also important for many classes.
Performing a Tip-in, Dunk or Layup is easier if the player can jump off of two legs and therefore the TwoLegged feature has a positive coefficient for those classes.
The Other class has a mix of shots where the player might jump off of one or two legs and as such has a low coefficient for TwoLegged.


\section{Outperformance of ordinal regression}
To showcase the outperformance of ordinal regression over multinomial regression we created a new data generating process such that:
The target variables are integers between 0 and 3, the feature matrix $X$ contains 12 features, each drawn from a standard normal distribution.
Generating a training set of 50 samples and a test set of 1000 samples we trained both a multinomial and an ordinal logistic regression model.
The results are shown in Table \ref{tab:results}.
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        Model & Accuracy & Log-loss\\
        \hline
        Multinomial & 0.323 & 1.708\\
        Ordinal & 0.339 & 1.433\\
        \hline
    \end{tabular}
    \caption{Results of the data generating process.}
    \label{tab:results}
\end{table}
We can see that Ordinal regression has a lower Log-loss value, this is due to the fact that extreme values in the features have higher chances to be classified either into 0 or 3 as these are the most extreme classes.
While the multinomial regression model there is no such clear separation and such values can be classified into any of the 4 classes.


\section{Conclusion}
Data in the real world is rearly binary and as such multinomial regression models are needed to classify data.
Likewise, ordinal regression models can be very useful for situations where modeling human preferences is needed.
This report showcased that the implementation of both models is relatively simple and that the coefficients of the models can be interpreted to gain insight into the data.

\bibliographystyle{IEEEtran}
\bibliography{main}

\end{document}
